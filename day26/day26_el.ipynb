{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 集成学习介绍\n",
    "\n",
    "- 集成学习：\n",
    "    - 利用一些算法，把已经存在的机器学习算法结合在一起；\n",
    "    - 特点：\n",
    "        - 存在已知算法：预测，分类；\n",
    "        - 结合已有算法；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 集成的思想与算法具体实现\n",
    "    - 数据集\n",
    "        - 通过数据集把相同、或者不同的算法结合在一起（利用数据的差异）\n",
    "    - 算法\n",
    "        - 对于相同或者不相同的数据集，把不同的算法结合在一起。（利用算法的差异）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  集成学习的总的思路"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 特别注意：\n",
    "    - 集成学习不是基于算法（函数或者类）的结合，而是算法与数据形成的学习器（分类器或者回归器：对象【调用过fit函数，训练过的对象】】）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 集成的主要目标：\n",
    "    - 集成以后的学习器的效果，要好于单个学习器。\n",
    "    - 被集成的学习器：基学习器（Base Learner：弱学习器Weaker）\n",
    "    - 最后集成的学习器：强学习器\n",
    "    - Stacking算法的：二次学习器：元（Meta  Learner）学习器；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 集成的思路\n",
    "    - Bootstrap Aggregating:自助聚合（Bagging）\n",
    "        - 采用不同的数据集，使用相同的算法（可以使用不同算法），形成多个若学习器，然后采用结合策略，形成强学习器\n",
    "    - Boosting 提升集成\n",
    "        - 采用不同的数据集，使用相同的算法（可以使用不同算法），形成多个弱学习器。然后采用结合策略，形成强学习器\n",
    "        \n",
    "    - Stacking堆叠集成\n",
    "        - 数据集相同，算法不同，形成多个弱学习器，然后采用结合策略，形成强学习器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 集成的目标与手段：\n",
    "    - boosting：减少残差（偏差）\n",
    "        - AdaBoosting\n",
    "        - GradientBoost：GBDT用于决策树的GradientBoost\n",
    "    - Bagging：减少方差\n",
    "        - 随机森林\n",
    "    - Stacking：提升识别率，预测准确度（sklearn中好像没有提供）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 结合策略\n",
    "    - 平均法\n",
    "        - 弱学习的结果值，直接平均（加权平均）作为最后的输出。\n",
    "    - 投票法\n",
    "        - 把某个样本的每个弱学习器的识别结果统计，分类最多的输出结果就是最终输出结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn中集成学习算法的应用\n",
    "\n",
    "```python\n",
    "    ensemble.AdaBoostClassifier([…])\tAn AdaBoost classifier.\n",
    "    \n",
    "    ensemble.BaggingClassifier([base_estimator, …])\tA Bagging classifier.\n",
    "\n",
    "    ensemble.ExtraTreesClassifier([…])\tAn extra-trees classifier.\n",
    "\n",
    "    ensemble.GradientBoostingClassifier([loss, …])\tGradient Boosting for classification.\n",
    "\n",
    "    ensemble.IsolationForest([n_estimators, …])\tIsolation Forest Algorithm\n",
    "    \n",
    "    ensemble.RandomForestClassifier([…])\tA random forest classifier.\n",
    "    \n",
    "    ensemble.VotingClassifier(estimators[, …])\tSoft Voting/Majority Rule classifier for unfitted estimators.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "识别正确数：28，识别正确率： 93.33%\n",
      "识别正确数：28，识别正确率： 93.33%\n",
      "识别正确数：28，识别正确率： 93.33%\n",
      "识别正确数：26，识别正确率： 86.67%\n",
      "识别正确数：28，识别正确率： 93.33%\n",
      "识别正确数：29，识别正确率： 96.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn.datasets as ds\n",
    "import sklearn.ensemble as en\n",
    "import sklearn.model_selection as ms\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# 实现一个训练与测试方法（数据集加载，数据集切分）\n",
    "data, target = ds.load_iris(return_X_y=True)\n",
    "data_train, data_test, target_train, target_test = ms.train_test_split(data, target, test_size=0.2)\n",
    "\n",
    "# 创建集成强学习器\n",
    "classifier_bagging = en.BaggingClassifier(n_estimators=100)\n",
    "classifier_extra = en.ExtraTreesClassifier(n_estimators=100)\n",
    "classifier_rf =  en.RandomForestClassifier(n_estimators=100)\n",
    "classifier_ada =  en.AdaBoostClassifier(n_estimators=100)\n",
    "classifier_gradient =  en.GradientBoostingClassifier(n_estimators=100)\n",
    "\n",
    "# 投票学习器\n",
    "# 线性回顾\n",
    "# 逻辑回归\n",
    "c1 = LogisticRegression(solver='lbfgs',multi_class='auto')\n",
    "# 支撑向量机\n",
    "c2 = SVC(C=1.0, kernel='rbf',gamma='scale')\n",
    "# 集成算法（KNN）\n",
    "\n",
    "\n",
    "classifier_vote =  en.VotingClassifier(estimators=[('c1', c1),('c2',c2 )])   # 需要多个学习器\n",
    "\n",
    "\n",
    "def  test_classifier(classifier):\n",
    "    # 使用训练与测试方案，观察训练与测试效果。\n",
    "    total_num = len(target_test)\n",
    "    # 训练\n",
    "    classifier.fit(data_train, target_train)\n",
    "    # 测试\n",
    "    pre = classifier.predict(data_test)\n",
    "    # 统计打印数据\n",
    "    correct_num = (pre ==target_test).sum()\n",
    "    # 打印\n",
    "    print(F'识别正确数：{correct_num}，识别正确率：{100.0*correct_num/total_num:6.2f}%')\n",
    "\n",
    "test_classifier(classifier_bagging)\n",
    "test_classifier(classifier_extra)\n",
    "test_classifier(classifier_rf)\n",
    "test_classifier(classifier_ada)\n",
    "test_classifier(classifier_gradient)\n",
    "test_classifier(classifier_vote) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 集成学习的算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging集成算法\n",
    "\n",
    "### 算法的过程\n",
    "- Bootstrap Aggregating\n",
    "    - 通过随机抽样，形成多个数据集。（抽样方法有很多种，其中存在一种抽样方式：Bootstrap抽样）\n",
    "    - 每个数据集训练出一个弱学习器。\n",
    "    - 对测试样本，，每个若学习器得到一个预测效果，\n",
    "    - 对多个预测效果进行投票。作为最后强学习器的预测结果。\n",
    "\n",
    "- 只有一个，如果应用到决策树：\n",
    "    - 随机森林"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 算法的关键\n",
    "\n",
    "- Bootstrap随机抽样法的作用\n",
    "    - 解决了小样本数据集的问题。\n",
    "    \n",
    "- Bootstrap随机抽样方法：\n",
    "    - 假设样本$X = \\{ x_1, x_2,\\cdots, x_n \\}$，$n$是样本容量。\n",
    "    - 抽样的数量也是$n$个。\n",
    "    - 抽样的方式是：有放回抽样\n",
    "        - 抽取的n个样本的训练集其中存在重复样本。\n",
    "- Bootstrap随机抽样的数学依据：\n",
    "    - 对鸢尾花数据集抽样m次，得到Bootstrap抽样数据集m个。\n",
    "    - 训练的结果：结构正确的置信度可以通过bootsrap抽样计算出来：Bagging算法投票的预测结果，置信度是95%。\n",
    "    \n",
    "    - m个Bootstrap数据集训练出来的基学习器的预测结果是m个，对m结果做频次统计，预测正确的可信度95%（是0的预测为0，1的预测为1），"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 改算法固定在决策树上\n",
    "    - 随机森林。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  bootstrap_sample(data , target):\n",
    "    data_sample = []\n",
    "    target_sample = []\n",
    "    # 得到data的长度n\n",
    "    # 循环n次\n",
    "    # 取0~n-1之间的均匀随机整数。\n",
    "    # 使用整数作为下标，得到采样数据\n",
    "    \n",
    "    # 返回所有的采样数据\n",
    "    return data_sample, target_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果算法使用决策树，就是随机森林。\n",
    "class Bagging:\n",
    "    \n",
    "    base_learners = []\n",
    "    base_num =20\n",
    "    def fit(data_train, target_train):\n",
    "        # 根据base_num基学习器的个数，调用bootstrap采样，得到base_num个数据集\n",
    "        # 使用指定的学习器类，循环base_num次构造多个学习器base_learners。\n",
    "        # 循环使用不同的数据集，训练base_learners中的对象\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def predict(data_test):\n",
    "        # 使用上面训练出来的base_num个学习器，预测data_test。\n",
    "        # 得到base_num个结果\n",
    "        # 对base_num个结果投票，作为最后的返回值。\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting集成算法\n",
    "\n",
    "- 与Bag一样都是在数据集做文章，尽量采用相同的算法：\n",
    "    - 直接原始数据集加权；得到新的数据集；得到新的结果。\n",
    "    \n",
    "    - 最终得到一个强学习器\n",
    "        - 采用误差损失函数最小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoosting算法\n",
    "    - 加权计算方式1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 预测模型\n",
    "\n",
    "- 弱学习器怎么结合成强学习器\n",
    "    - 通过m次变换权重，得到m个数据集$X_i$\n",
    "    - 训练出m个基学习器$G_i(x)$\n",
    "    - 采用平均的值对方式得到最终的结果。\n",
    "    - $f(x) =\\alpha_1 G_1(x) + \\alpha_2 G_2(x) + \\cdots +\\alpha_m G_m(x)$\n",
    "    - $f(x) = \\sum \\limits _{i=1} ^{m} \\alpha_i G_i(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 损失模函数\n",
    "\n",
    "- $L(y, f(x)) = \\sum \\limits_{i=1} ^ {n} L(y_i, f(x_i))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 一般传统的度量方式：$L(y_i, f(x_i)) =\\sqrt{ {(y_i - f(x_i))}^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- AdaBoosting的误差度量：采用是指数函数\n",
    "\n",
    "    - $L(y,  f(x)) = e^{-yf(x)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $L(x, f(x)) = \\sum \\limits_{i=1} ^{n} e^{y_i \\sum \\limits _{k=1}^{m} \\alpha_k G_k(x_i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Ada损失模型的简化模型\n",
    "\n",
    "- 多元简化成一元"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 简化预测模型：\n",
    "    - $G_1(x)$：认为所有样本的重要性一样，取权重$\\dfrac{1}{n}$,每个样本乘以权重，得到新的样本集$\\bar{X}$\n",
    "    - $f_1= \\alpha_1 G_1(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $f_k (x) = f_{k-1}(x) + \\alpha_k G_k(x) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 损失模型简化：\n",
    "    - $L(\\alpha_k, G_k(x), w_k) = \\sum \\limits _{i=1} ^{n} e ^{y_i f_k(x_i)} =  \\sum \\limits _{i=1} ^{n} e ^{y_i (f_{k-1} (x_i) + \\alpha_k G_k(x_i))}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoosting算法过程\n",
    "- 下面算法过程中的公式全部来自上面的损失函数最小值推导的公式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 假设：\n",
    "     - 样本集$X = \\{ (x_1, y_1), (x_2, y_2), \\cdots, (x_n, y_n) \\}$\\\n",
    "     - $x_i \\in R$\n",
    "     - $y_i \\in \\{-1, 1\\}$, 把问题假设二分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 初始化：\n",
    "    - 初始化一个权重：$(w_1, w_2, \\cdots, w_n)$, $w_i= \\dfrac{1}{n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 得到训练样本集\n",
    "    - $X_1 = \\{(w_1 x_1, y_1), \\cdots, (w_n x_n, y_n)\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 使用指定的算法与数据集$X_1$训练得到基学习器$G_1(x) : x \\to \\{-1, 1\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 计算错误率\n",
    "     - $e_1 = \\sum _{i=1} ^{n} w_i I(G_1(x_i))$\n",
    "         - 其中$I$函数的定义:$G(x_i)$是错误，$I$返回1，否则返回0\n",
    "         - $\\sum \\limits _ {i=1} ^n w_i   =1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 计算$\\alpha_1$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $ \\alpha_1 = \\dfrac{1}{2} ln (\\dfrac{1 - e_1}{e_1} )$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. 计算下一次的权重系数$(w_1 ^{(2)}, \\cdots,w_n ^{(2)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 权重系数的计算步骤：\n",
    "    1. 计算规范化因子：\n",
    "        - $z_1 = \\sum \\limits _{i=1} ^ n w_i^{(1)} e ^{ - \\alpha_1 y_i G_1(x_i)}$\n",
    "        \n",
    "    2. $w_i^{(2)} = \\dfrac{w_i^{(1)} e ^{ \\alpha_1 y_i G_1(x_1)} }{z_1} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 注意：\n",
    "    - $\\sum \\limits _{i=1} ^{n} w_i^{(2)} = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. 使用$w_i^{(2)}$得到新的数据集，训练$G_2(x)$，然后开始循环到m次结束，"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradientBoosting算法\n",
    "    - 加权计算方式2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking集成算法\n",
    "    - 在算法上做文章。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
