{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、决策树应用体验  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 分类   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4) (150,)\n",
      "50\n",
      "50\n",
      "50\n",
      "1.0\n",
      "[2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn  import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# 认知红酒数据集（178个样本，3类红酒，每类样本数 ( 59, 71, 48 ) ）\n",
    "data,target=datasets.load_iris(return_X_y=True)\n",
    "#data,target=datasets.load_wine(return_X_y=True)\n",
    "print(data.shape,target.shape)\n",
    "print((target==0).sum())\n",
    "print((target==1).sum())\n",
    "print((target==2).sum())\n",
    "\n",
    "# ============================\n",
    "clf = DecisionTreeClassifier()\n",
    "clf = clf.fit(data[0:145], target[0:145])\n",
    "# ============================\n",
    "\n",
    "# 分类结果\n",
    "pre_cls = clf.predict( data )\n",
    "# 分类概率\n",
    "pre_pro = clf.predict_proba(data)\n",
    "# 分类准确率\n",
    "score = clf.score(data, target)\n",
    "# print(pre_cls) \n",
    "# print(pre_pro) \n",
    "print(score)\n",
    "# path=clf.decision_path(data)\n",
    "# print(path)\n",
    "print(clf.predict( data[145:150]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;从上面可以看出，决策树对分类具有线性回归无可比拟的优势, 如果对未参与训练的数据集是否也有这么好的分类效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 3)\n",
      "(20, 3)\n",
      "[[193.  36.  46.]]\n",
      "[193.  36.  46.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn  import datasets\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "# 体能数据physiological Data（Weight重, Waist腰 and Pulse脉搏.），exercise Target（Chins下巴, Situps仰卧起坐 and Jumps跳跃.）\n",
    "data,target=datasets.load_linnerud(return_X_y=True)\n",
    "print(data.shape)\n",
    "print(target.shape)\n",
    "clf = DecisionTreeRegressor()\n",
    "clf = clf.fit(data[0:15], target[0:15])\n",
    "pre=clf.predict([data[14]])\n",
    "\n",
    "print(pre)\n",
    "print(target[14])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、决策树的数学基础"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 决策与信息论    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  从决策开始谈起      \n",
    "人类经常面临各种决策，比如到了周末，准备安排下自己的周末活动，就会使用到决策：     \n",
    "&emsp;&emsp;---->公司老板是否有约------------------------------------------有---**【加班】**     \n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;|---->女朋友是否有约-------------------------------有---**【嗨皮】**     \n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;|---->朋友是否有约打球----------------有---**【锻炼】**     \n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;|-----------------------------无---**【游戏】**        \n",
    "&emsp;&emsp;     \n",
    "上述决策中，抽象成一般的数学描述是：       \n",
    "&emsp;&emsp;|-一个人的分类结果：加班，嗨皮，锻炼，游戏        \n",
    "&emsp;&emsp;|-分类的数据行为数据：\\[公司老板是否，女朋友是否有约，朋友是否有约球\\]，我们称为样本数据，数据中的三个情况，称为数据特征。      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;&emsp;&emsp;如果使用鸢尾花数据应该更加细致一点说明：四个特征数据$(x_1,x_2,x_3,x_4)$          \n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;【100样本(50A类:50B类)】        \n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;|条件：$(x1 >= 2.1)$        \n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;【60样本(40A类:20B类)】-----【100样本(30A类:10B类)】     \n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;|$(x2>3.5)$&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;|$(x2>4.0)$  \n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;......&emsp;&emsp;【100样本(25A类:0B类)】---【100样本(5A类:10B类)】     \n",
    "&emsp;&emsp;       \n",
    "&emsp;&emsp;循环直到样本都属于某一类，最后分类成功。上面分类产生的决策结构就是**决策树**(一种形象的称呼)           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 决策的核心要素          \n",
    "决策中有两个核心要素：        \n",
    "&emsp;&emsp;|-（1）选择合适的特征          \n",
    "&emsp;&emsp;|-（2）分类的阈值       \n",
    "&emsp;&emsp;       \n",
    "&emsp;&emsp;决策的结果：**决策树**    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.  决策的数学知识       \n",
    "其中选取哪个特征，特征的阈值怎么取，依赖一个数学分支学科：**信息论**。\n",
    "&emsp;&emsp;      \n",
    "**信息论**是运用概率论与数理统计的方法研究信息、信息熵、通信系统、数据传输、密码学、数据压缩等问题的应用数学学科。信息论将信息的传递作为一种统计现象来考虑。\n",
    "信息论之父：克劳德·香农（Claude Shannon）       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 决策分类与信息论      \n",
    "决定决策特征与分类阈值的数学过程就是决策树算法，该算法使用信息论理论知识。其中最基础的算法称为ID3算法。该算法还衍生出**C4.5，C5.0和CART**算法。 \n",
    ">**ID3（Iterative Dichotomiser 3）算法**也称迭代二分法，是一种贪心算法，用来构造决策树。ID3算法起源于概念学习系统（CLS），以**信息熵**的下降速度为选取测试属性的标准，即在每个节点选取还尚未被用来划分的具有**最高信息增益的**特征作为划分标准，然后继续这个过程，直到生成的决策树能完美分类训练样例。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 决策树基本概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 不纯度     \n",
    "&emsp;&emsp;当在某个节点上，样本不是属于单一的一个类别，我们称为样本不纯，度量样本不纯的概念称为不纯度。 当某个节点上样本都属于某一类，该节点的样本的不纯度为0。        \n",
    "&emsp;&emsp;      \n",
    "&emsp;&emsp;度量不纯度有三种方式：         \n",
    "&emsp;&emsp;&emsp;&emsp;|- 基尼指数：Gini index($I_G$)      \n",
    "&emsp;&emsp;&emsp;&emsp;|- 信息熵：Entropy($I_H$)      \n",
    "&emsp;&emsp;&emsp;&emsp;|- 分类误差：Classification Error($I_E$)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 基尼指数（Gini  Index）     \n",
    "&emsp;&emsp; 假设在某个决策节点，有$n$个样本，$m$个类别。每个类别的概率为$p_i$：     \n",
    "&emsp;&emsp;&emsp;&emsp;|- $I_G=\\sum \\limits _{i=1}^{m} p_i(1-p_i)=1-\\sum \\limits _{i=1}^{m} {p_i}^2$         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 信息熵（Entropy）     \n",
    "&emsp;&emsp; 假设在某个决策节点，有$n$个样本，$m$个类别。每个类别的概率为$p_i$：     \n",
    "&emsp;&emsp;&emsp;&emsp;|-$I_H = - \\sum \\limits _{i=1}^{m} p_i log(p_i) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "备注： 当年Claude Shannon提出这个公式的时候，冯诺依曼（John von Neumann）建议给一个名字为：熵（Entropy：平均信息值）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 分类误差（Classification Error）      \n",
    "&emsp;&emsp; 假设在某个决策节点，有$n$个样本，$m$个类别。每个类别的概率为$p_i$：   \n",
    "&emsp;&emsp;&emsp;&emsp;|- $ I_E = 1-  \\overset{m}{\\underset{i=1}{\\text{Max}}} \\  p_i $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 概率与占比     \n",
    "&emsp;&emsp;上述度量公式中使用的概率，可以使用样本在每个决策节点中的类别占比来代替：     \n",
    "&emsp;&emsp; 假设在某个决策节点，有$n$个样本$(x_1,x_2,\\dots,x_n)$，$m$个类别{$C_1,C_2,\\dots,C_m$}。每个类别的概率为$p_i$可以使用占比替代：  \n",
    "&emsp;&emsp;&emsp;&emsp;|- $p_i= \\dfrac{1}{n} \\sum \\limits _{x_j \\in C_i} 1$，其实就是：每类总数 / 样本总数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 信息增益     \n",
    "&emsp;&emsp;信息增益用来决定分类的阈值，每个算法的信息增益值不一样，下面在具体的算法中给不不同算分信息增益计算公式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 决策树模型 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  生成决策树     \n",
    "&emsp;&emsp;|- (1)计算信息熵       \n",
    "&emsp;&emsp;|- (2)计算信息增益      \n",
    "&emsp;&emsp;|- (3)根据信息增益，选择分类特征；      \n",
    "&emsp;&emsp;|- (4)计算特征的不纯度，不纯度为0，就是叶子节点，不在继续分类，不纯度不为0，则继续（1）步骤，直到所有样本的不纯度都是0，就是被正确分类。        \n",
    "&emsp;&emsp;      \n",
    "两个关键项：    \n",
    "&emsp;&emsp;|-**信息增益值**确定分类特征  。       \n",
    "&emsp;&emsp;|-**不纯度**确定子节点产生（ 纯与不纯 ），确定分类的**阈值**  。       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 决策模型       \n",
    "直接输入分类特征，按照决策树分类，到叶子节点的结果就是最终的分类输出。          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三、决策树算法验证与数学基础          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 决策树算法验证"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 过拟合缺陷体验          \n",
    "下面代码用来体验，决策树的过拟合缺点。尽管决策树有容易理解，算法容易实现，计算量小等优点，但他有个缺陷就是：过拟合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 4)\n",
      "(90,)\n",
      "(60, 4)\n",
      "(60,)\n",
      "预测评估： 0.9333333333333333\n",
      "预测评估： 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split    # 训练集切分\n",
    "\n",
    "data,target=datasets.load_iris(return_X_y=True)\n",
    "train_data, test_data,train_target, test_target = train_test_split(data, target, test_size=0.4,shuffle=True)\n",
    "print(train_data.shape)\n",
    "print(train_target.shape)\n",
    "print(test_data.shape)\n",
    "print(test_target.shape)\n",
    "\n",
    "dtc=DecisionTreeClassifier()\n",
    "# 使用训练样本训练\n",
    "dtc.fit(train_data,train_target)\n",
    "# 使用测试集测试\n",
    "score=dtc.score(test_data,test_target)\n",
    "print('预测评估：',score)\n",
    "\n",
    "# 使用全部样本训练\n",
    "dtc.fit(data,target)\n",
    "# 使用全部样本测试\n",
    "score=dtc.score(data,target)\n",
    "print('预测评估：',score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 决策树属性的理解体验   \n",
    "下面代码输出决策树的属性，可以结合上面决策树的模型去理解。   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分类类别： [0 1 2]\n",
      "特征的重要性： [0.         0.01333333 0.06405596 0.92261071]\n",
      "最大特征数： 4\n",
      "类别数： 3\n",
      "训练时的特征数： 4\n",
      "训练输出数据的维数： 1\n",
      "评估： 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split    # 训练集切分\n",
    "\n",
    "data,target=datasets.load_iris(return_X_y=True)\n",
    "#data,target=datasets.load_linnerud(return_X_y=True)\n",
    "\n",
    "\n",
    "dtc=DecisionTreeClassifier(random_state=0,min_samples_split=2)    # 为了保证输出结果的稳定性，需要random_state固定。\n",
    "dtc.fit(data,target)                  #训练\n",
    "\n",
    "print('分类类别：',dtc.classes_)\n",
    "print('特征的重要性：',dtc.feature_importances_ )\n",
    "print('最大特征数：',dtc.max_features_)\n",
    "print('类别数：',dtc.n_classes_)\n",
    "print('训练时的特征数：',dtc.n_features_)\n",
    "print('训练输出数据的维数：',dtc.n_outputs_)     # 一般对分类来说都是一个标量来判定，体能数据的输出就是3\n",
    "\n",
    "print('评估：',dtc.score(data,target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 决策树的树结构体验      \n",
    "通过决策树的结构体验，我们基本上知道决策事是怎么实现的，然后关注特征选择的信息熵的计算，以及不纯度的计算，我们就可以实现决策树。     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "结点数：\n",
      " 17\n",
      "容量：\n",
      " 17\n",
      "最大深度：\n",
      " 5\n",
      "节点分类的特征索引：-2表示没有分类特征，就是叶子节点：\n",
      " [ 3 -2  3  2  3 -2 -2  3 -2  2 -2 -2  2  1 -2 -2 -2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split    # 训练集切分\n",
    "import sklearn\n",
    "\n",
    "data,target=datasets.load_iris(return_X_y=True)\n",
    "#data,target=datasets.load_linnerud(return_X_y=True)\n",
    "\n",
    "\n",
    "dtc=DecisionTreeClassifier(random_state=0,min_samples_split=2)    # 为了保证输出结果的稳定性，需要random_state固定。\n",
    "dtc.fit(data,target)  \n",
    "tree=dtc.tree_\n",
    "\n",
    "# 打印树的帮助\n",
    "# help(sklearn.tree._tree.Tree)\n",
    "\n",
    "print('结点数：\\n',tree.node_count)\n",
    "print('容量：\\n',tree.capacity)\n",
    "print('最大深度：\\n',tree.max_depth)\n",
    "print('节点分类的特征索引：-2表示没有分类特征，就是叶子节点：\\n',tree.feature)     #-2表示是叶子节点（因为没有分类特征）\n",
    "# 说明：3表示使用的第4个特征，进行子节点分叉。-2表示叶子节点，没有子节点分叉。   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "节点的样本总数：\n",
      " [150  50 100  54  48  47   1   6   3   3   2   1  46   3   2   1  43]\n"
     ]
    }
   ],
   "source": [
    "print('节点的样本总数：\\n',tree.n_node_samples)   \n",
    "#  150表示决策树上每个节点，未分叉样本数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "节点每类样本数：\n",
      " [[[50. 50. 50.]]\n",
      "\n",
      " [[50.  0.  0.]]\n",
      "\n",
      " [[ 0. 50. 50.]]\n",
      "\n",
      " [[ 0. 49.  5.]]\n",
      "\n",
      " [[ 0. 47.  1.]]\n",
      "\n",
      " [[ 0. 47.  0.]]\n",
      "\n",
      " [[ 0.  0.  1.]]\n",
      "\n",
      " [[ 0.  2.  4.]]\n",
      "\n",
      " [[ 0.  0.  3.]]\n",
      "\n",
      " [[ 0.  2.  1.]]\n",
      "\n",
      " [[ 0.  2.  0.]]\n",
      "\n",
      " [[ 0.  0.  1.]]\n",
      "\n",
      " [[ 0.  1. 45.]]\n",
      "\n",
      " [[ 0.  1.  2.]]\n",
      "\n",
      " [[ 0.  0.  2.]]\n",
      "\n",
      " [[ 0.  1.  0.]]\n",
      "\n",
      " [[ 0.  0. 43.]]]\n"
     ]
    }
   ],
   "source": [
    "print('节点每类样本数：\\n',tree.value)    \n",
    "# [[50.  0.  0.]] 表述决策树上，，节点的样本所属类别（标签）的样本数：A类50个，B类0个，C类0个，这是一个不纯度为0的节点，是叶子节点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分类阈值：-2表示叶子节点，不在分子节点：\n",
      " [ 0.80000001 -2.          1.75        4.94999981  1.6500001  -2.\n",
      " -2.          1.54999995 -2.          5.44999981 -2.         -2.\n",
      "  4.85000038  3.0999999  -2.         -2.         -2.        ]\n"
     ]
    }
   ],
   "source": [
    "print('分类阈值：-2表示叶子节点，不在分子节点：\\n',tree.threshold)\n",
    "# -2表示没有分叉 ，是叶子节点，阈值就是-2表示。每个样本分叉就按照阈值分叉，这儿每个样本应该有一个信息熵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "节点的不纯度：\n",
      " [0.66666667 0.         0.5        0.16803841 0.04079861 0.\n",
      " 0.         0.44444444 0.         0.44444444 0.         0.\n",
      " 0.04253308 0.44444444 0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "print('节点的不纯度：\\n',tree.impurity) \n",
    "# 不纯度为0，表示叶子节点，不会分叉。不纯度非0，表示需要分叉，分类按照每个样本的阈值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "左节点：-1表示叶子节点：\n",
      " [ 1 -1  3  4  5 -1 -1  8 -1 10 -1 -1 13 14 -1 -1 -1]\n",
      "右节点：-1表示叶子节点：\n",
      " [ 2 -1 12  7  6 -1 -1  9 -1 11 -1 -1 16 15 -1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "print('左节点：-1表示叶子节点：\\n',tree.children_left)   \n",
    "print('右节点：-1表示叶子节点：\\n',tree.children_right) \n",
    "# 列出决策树的每个节点的id（按照顺序排列）-1表示叶子节点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 决策树的树结构可视化       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split    # 训练集切分\n",
    "import sklearn\n",
    "from sklearn import tree\n",
    "from sklearn.externals.six import StringIO    #数据缓冲\n",
    "import pydot    # 把数据转换为图像\n",
    "\n",
    "data,target=datasets.load_iris(return_X_y=True)\n",
    "#data,target=datasets.load_linnerud(return_X_y=True)\n",
    "\n",
    "\n",
    "dtc=DecisionTreeClassifier(random_state=0,criterion='entropy')    # 为了保证输出结果的稳定性，需要random_state固定。\n",
    "dtc.fit(data,target)  \n",
    "\n",
    "buff = StringIO()\n",
    "# tree.export_graphviz(dtc,out_file=buff)     # export_graphviz只能导出数据\n",
    "tree.export_graphviz(dtc, out_file=buff, feature_names=['特征1','特征2','特征3','特征4'], class_names=['A类','B类','C类'], filled=True, rounded=True, special_characters=True)  \n",
    "\n",
    "(graph,)= pydot.graph_from_dot_data(buff.getvalue())   # 转换为图像\n",
    "graph.write_svg(\"iris.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "显示上面产生的图片。![image.png](iris.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. ID3决策算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 样本总体信息熵      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp; 假设在某个决策节点，有$n$个样本，$m$个类别。每个类别的概率为$p_i$：     \n",
    "&emsp;&emsp;&emsp;&emsp;|-$I_H = - \\sum \\limits _{i=1}^{m} p_i log(p_i) $     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp; 比如鸢尾花数据集一共**150**个，共分**3**类，按照如下步骤计算信息熵 ：       \n",
    "&emsp;&emsp;&emsp;&emsp;|- （1）计算样本属于每类的概率为： $p_1=50/150=\\dfrac{1}{3},p_2=50/150=\\dfrac{1}{3},p_3=50/150=\\dfrac{1}{3}$       \n",
    "&emsp;&emsp;&emsp;&emsp;|- （2）计算信息熵为：$I_H=- (\\dfrac{1}{3} log\\dfrac{1}{3} + \\dfrac{1}{3} log \\dfrac{1}{3} + \\dfrac{1}{3} log \\dfrac{1}{3})$     \n",
    "&emsp;&emsp;      \n",
    "&emsp;&emsp; 样本总体信息熵与训练特征无关，只与标签有关，具体代码如下：    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.584962500721156, dict_items([(0, 50), (1, 50), (2, 50)])]\n"
     ]
    }
   ],
   "source": [
    "from sklearn  import datasets\n",
    "import numpy as np\n",
    "def calcute_total_entropy(train_label):\n",
    "    # train_label：总体样本的输出标签\n",
    "\n",
    "    # 获取总得样本数\n",
    "    total_num=len(train_label)\n",
    "    # 循环得到所有类别的样本总数\n",
    "    label_num={}\n",
    "    for data in train_label:\n",
    "        if  data not in label_num:\n",
    "            label_num[data]=0\n",
    "        label_num[data]+=1\n",
    "    # 计算每类的概率\n",
    "    entropy=0.0\n",
    "    for  cls,num in label_num.items():\n",
    "        prop=1.0*num/total_num\n",
    "        entropy+=prop *np.log2(prop)\n",
    "    return [-entropy,label_num.items()]\n",
    "\n",
    "data,target=datasets.load_iris(return_X_y=True)\n",
    "ent=calcute_total_entropy(target)\n",
    "print(ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.5849625  0.         1.         0.44506486 0.14609425 0.\n",
      " 0.         0.91829583 0.         0.91829583 0.         0.\n",
      " 0.15109697 0.91829583 0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn  import datasets\n",
    "import numpy as np\n",
    "dtc=DecisionTreeClassifier(criterion='entropy')\n",
    "# 使用训练样本训练\n",
    "dtc.fit(data,target)\n",
    "print(dtc.tree_.impurity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 注意：&emsp;&emsp;通过对我们与sklearn实现的结果比较，发现sklearn的对数运算使用的是以2为底的对数运算。    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 样本特征值信息熵      \n",
    "&emsp;&emsp;样本总体信息熵是所有样本按照标签值作为特征计算信息熵（分类标准按照标签分类），样本特征信息熵就是按照训练样本的某个特征值来计算信息熵（分类标准按照标签分类） ，一个特征值一个信息熵，使用这些特征熵就可以计算条件熵。    \n",
    "&emsp;&emsp;      \n",
    "&emsp;&emsp;样本特征值信息熵，就是某个特征中某个值的信息熵，比如鸢尾花中4个特征，第一个特征150个取值，其中有一个取值值一样有20个，其中A类6个，B类9个，C类5个，这样就可以求出这个20个相同特征值的信息熵：$H_{20}=\\dfrac{6}{20} log \\dfrac{6}{20} + \\dfrac{9}{20} log \\dfrac{9}{20} + \\dfrac{5}{20} log \\dfrac{5}{20}$， 下面通过例子来说明：     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.34883209584303193, 9, dict_items([(0, 8), (1, 1)])]\n"
     ]
    }
   ],
   "source": [
    "from sklearn  import datasets\n",
    "import numpy as np\n",
    "def calcute_value_entropy(data_train, data_label, index, value):\n",
    "    # data_train 训练样本\n",
    "    # data_label 样本标签\n",
    "    # index，特征索引，就是训练样本中的第几个特征，从0开始。\n",
    "    # value，需要计算信息熵的特征值。某些特征值在样本中只有一个。这种信息熵等于0，就是不纯度为0。\n",
    "    total_num=0   # 某个特征值在数据集中的个数\n",
    "    label_num={}  # 特征值按照分类统计，鸢尾花A类多少个，B类多少个，C类多少个\n",
    "    i=0                # 用来索引data_train与data_label对应位置的数据\n",
    "    # 下面只对index的特征处理，其他特征不处理\n",
    "    for data in data_train[:,index]:\n",
    "        # 只对指定的特征值进行统计\n",
    "        if data == value:\n",
    "            total_num+=1  # 特征值总数递增\n",
    "            # 判定类别\n",
    "            label=data_label[i]\n",
    "            if label not in  label_num:\n",
    "                label_num[label]=0\n",
    "            label_num[label]+=1\n",
    "            \n",
    "        # 标签对应位置下移\n",
    "        i+=1\n",
    "    # 统计特征值的分类个数，下面计算熵\n",
    "    entropy=0.0\n",
    "    for  cls,num in label_num.items():\n",
    "        prop=1.0*num/total_num\n",
    "        entropy+=prop *np.log2(prop)\n",
    "    return [-entropy,total_num,label_num.items()]\n",
    "data,target=datasets.load_iris(return_X_y=True)\n",
    "#print(data)\n",
    "ent=calcute_value_entropy(data,target,0,5.1)\n",
    "print(ent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;下面我们可以计算出鸢尾花某个特征的所有特征值的信息熵。    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "5.1 : [0.5032583347756457, 9, dict_items([(0, 8), (1, 1)])]\n",
      "4.9 : [1.2516291673878228, 6, dict_items([(0, 4), (1, 1), (2, 1)])]\n",
      "4.7 : [-0.0, 2, dict_items([(0, 2)])]\n",
      "4.6 : [-0.0, 4, dict_items([(0, 4)])]\n",
      "5.0 : [0.7219280948873623, 10, dict_items([(0, 8), (1, 2)])]\n",
      "5.4 : [0.6500224216483541, 6, dict_items([(0, 5), (1, 1)])]\n",
      "4.4 : [-0.0, 3, dict_items([(0, 3)])]\n",
      "4.8 : [-0.0, 5, dict_items([(0, 5)])]\n",
      "4.3 : [-0.0, 1, dict_items([(0, 1)])]\n",
      "5.8 : [1.4488156357251847, 7, dict_items([(0, 1), (1, 3), (2, 3)])]\n",
      "5.7 : [1.2987949406953985, 8, dict_items([(0, 2), (1, 5), (2, 1)])]\n",
      "5.2 : [0.8112781244591328, 4, dict_items([(0, 3), (1, 1)])]\n",
      "5.5 : [0.863120568566631, 7, dict_items([(0, 2), (1, 5)])]\n",
      "4.5 : [-0.0, 1, dict_items([(0, 1)])]\n",
      "5.3 : [-0.0, 1, dict_items([(0, 1)])]\n",
      "7.0 : [-0.0, 1, dict_items([(1, 1)])]\n",
      "6.4 : [0.863120568566631, 7, dict_items([(1, 2), (2, 5)])]\n",
      "6.9 : [0.8112781244591328, 4, dict_items([(1, 1), (2, 3)])]\n",
      "6.5 : [0.7219280948873623, 5, dict_items([(1, 1), (2, 4)])]\n",
      "6.3 : [0.9182958340544896, 9, dict_items([(1, 3), (2, 6)])]\n",
      "6.6 : [-0.0, 2, dict_items([(1, 2)])]\n",
      "5.9 : [0.9182958340544896, 3, dict_items([(1, 2), (2, 1)])]\n",
      "6.0 : [0.9182958340544896, 6, dict_items([(1, 4), (2, 2)])]\n",
      "6.1 : [0.9182958340544896, 6, dict_items([(1, 4), (2, 2)])]\n",
      "5.6 : [0.6500224216483541, 6, dict_items([(1, 5), (2, 1)])]\n",
      "6.7 : [0.954434002924965, 8, dict_items([(1, 3), (2, 5)])]\n",
      "6.2 : [1.0, 4, dict_items([(1, 2), (2, 2)])]\n",
      "6.8 : [0.9182958340544896, 3, dict_items([(1, 1), (2, 2)])]\n",
      "7.1 : [-0.0, 1, dict_items([(2, 1)])]\n",
      "7.6 : [-0.0, 1, dict_items([(2, 1)])]\n",
      "7.3 : [-0.0, 1, dict_items([(2, 1)])]\n",
      "7.2 : [-0.0, 3, dict_items([(2, 3)])]\n",
      "7.7 : [-0.0, 4, dict_items([(2, 4)])]\n",
      "7.4 : [-0.0, 1, dict_items([(2, 1)])]\n",
      "7.9 : [-0.0, 1, dict_items([(2, 1)])]\n"
     ]
    }
   ],
   "source": [
    "from sklearn  import datasets\n",
    "import numpy as np\n",
    "\n",
    "def calcute_feature_entropy(data_train, data_label, index):\n",
    "    # data_train 训练样本\n",
    "    # data_label 样本标签\n",
    "    # index，特征索引，就是训练样本中的第几个特征，从0开始。\n",
    "    label_entropy={}\n",
    "    # 对训练样本遍历，统计计算每个值的信息熵(只对某个特征值统计)\n",
    "    for data in data_train[ :, index]:\n",
    "        if data not in label_entropy:\n",
    "            ent=calcute_value_entropy(data_train, data_label,index,data)\n",
    "            label_entropy[data]=ent# 只使用前面两个值，其他值可以根据情况使用\n",
    "    return label_entropy\n",
    "\n",
    "data,target=datasets.load_iris(return_X_y=True)    \n",
    "ents=calcute_feature_entropy(data,target,0)\n",
    "print(len(ents))\n",
    "for k,v in ents.items():\n",
    "    print(k,\":\",v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 样本特征总体熵（条件熵）     \n",
    "&emsp;&emsp;样本特征总体上，就是固定某个特征的信息熵，为了更加容易理解，按照我们前面的定义，总体信息熵定义如下：    \n",
    "&emsp;&emsp;&emsp;&emsp;假设在某个决策节点，有$n$个样本，$m$个类别。每个类别的概率为$p(C_i)$，其中$C_i$表示属于某类别，$C$表示所有类别 ：     \n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;|-$I_H(C) = - \\sum \\limits _{i=1}^{m} p_i（C_i） log(p_i(C_i)) $       \n",
    "&emsp;&emsp;      \n",
    "&emsp;&emsp;我们可以把特征熵按照条件概率建模，特征信息熵定义如下：     \n",
    "&emsp;&emsp;&emsp;&emsp;假设某个特征的样本值不同的是$k$个，特征值集合表示为$(x_1,x_2,\\dots,x_k)$，则特征的总体熵（条件熵）定义如下：    \n",
    "&emsp;&emsp;&emsp;&emsp;|-$I_H(C | X) = - \\sum \\limits _{i=1}^{k} p_i H（C | X=x_i） $      \n",
    "&emsp;&emsp;      \n",
    "&emsp;&emsp;我们就可以计算出鸢尾花4个特征的信息熵（ 也称条件熵 ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7080248798300983\n",
      "1.07409253659755\n",
      "0.1386459770753561\n",
      "0.14906466204571436\n"
     ]
    }
   ],
   "source": [
    "from sklearn  import datasets\n",
    "import numpy as np\n",
    "\n",
    "# 计算某个特征中某个值的信息熵\n",
    "def calcute_value_entropy(data_train, data_label, index, value):\n",
    "    # data_train 训练样本\n",
    "    # data_label 样本标签\n",
    "    # index，特征索引，就是训练样本中的第几个特征，从0开始。\n",
    "    # value，需要计算信息熵的特征值。某些特征值在样本中只有一个。这种信息熵等于0，就是不纯度为0。\n",
    "    total_num=0   # 某个特征值在数据集中的个数\n",
    "    label_num={}  # 特征值按照分类统计，鸢尾花A类多少个，B类多少个，C类多少个\n",
    "    i=0                # 用来索引data_train与data_label对应位置的数据\n",
    "    # 下面只对index的特征处理，其他特征不处理\n",
    "    for data in data_train[:,index]:\n",
    "        # 只对指定的特征值进行统计\n",
    "        if data == value:\n",
    "            total_num+=1  # 特征值总数递增\n",
    "            # 判定类别\n",
    "            label=data_label[i]\n",
    "            if label not in  label_num:\n",
    "                label_num[label]=0\n",
    "            label_num[label]+=1\n",
    "            \n",
    "        # 标签对应位置下移\n",
    "        i+=1\n",
    "    # 统计特征值的分类个数，下面计算熵\n",
    "    entropy=0.0\n",
    "    for  cls,num in label_num.items():\n",
    "        prop=1.0*num/total_num\n",
    "        entropy+=prop *np.log2(prop)\n",
    "    return [-entropy,total_num,label_num.items()]\n",
    "\n",
    "#  计算某个特征的信息熵\n",
    "def calcute_feature_entropy(data_train, data_label, index):\n",
    "    # data_train 训练样本\n",
    "    # data_label 样本标签\n",
    "    # index，特征索引，就是训练样本中的第几个特征，从0开始。\n",
    "    \n",
    "    total_num=len(data_train)  # \n",
    "    label_entropy={}\n",
    "    # 对训练样本遍历，统计计算每个值的信息熵(只对某个特征值统计)\n",
    "    for data in data_train[ :, index]:\n",
    "        if data not in label_entropy:\n",
    "            ent=calcute_value_entropy(data_train, data_label,index,data)\n",
    "            label_entropy[data]=[ ent[0], ent[1] ]\n",
    "    \n",
    "    # 计算整个特征的信息熵\n",
    "    feature_entropy=0.0\n",
    "    for _, v in  label_entropy.items():\n",
    "        prop=1.0 * v[1] / total_num\n",
    "        feature_entropy+=prop * v[0]\n",
    "    return feature_entropy\n",
    "\n",
    "# 计算鸢尾花四个特征的信息熵\n",
    "data,target=datasets.load_iris(return_X_y=True)    \n",
    "feature_num=data.shape[1]  # 特征数\n",
    "for idx  in range(feature_num):\n",
    "    ent=calcute_feature_entropy(data,target,idx)\n",
    "    print(ent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 特征信息增益     \n",
    "&emsp;&emsp;所谓信息增益值，是针对样本总体信息熵而言，起定义如下：        \n",
    "&emsp;&emsp;&emsp;&emsp;假设训练样本集有$N$个特征，$i$表示样本特征的索引：$0 \\le i \\le N$。$X_i$就表示第$i$个特征。则第$i$个特征$X_i$的信息增益表示如下：            \n",
    "&emsp;&emsp;&emsp;&emsp;|-$IG(X) =I_H(C)-I_H(C|X_i)$      \n",
    "&emsp;&emsp;      \n",
    "&emsp;&emsp;下面使用代码来说明鸢尾花的特征信息增益的计算结果（为了保证大家的阅读方便，我把所有代码汇集在一起形成一个独立完整的程序）。    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8769376208910578, 0.5108699641236061, 1.4463165236458, 1.4358978386754417]\n"
     ]
    }
   ],
   "source": [
    "from sklearn  import datasets\n",
    "import numpy as np\n",
    "\n",
    "# 计算总体信息熵========================================\n",
    "def calcute_total_entropy(train_label):\n",
    "    # train_label：总体样本的输出标签\n",
    "    # 获取总得样本数\n",
    "    total_num=len(train_label)\n",
    "    # 循环得到所有类别的样本总数\n",
    "    label_num={}\n",
    "    for data in train_label:\n",
    "        if  data not in label_num:\n",
    "            label_num[data]=0\n",
    "        label_num[data]+=1\n",
    "    # 计算每类的概率\n",
    "    entropy=0.0\n",
    "    for  cls,num in label_num.items():\n",
    "        prop=1.0*num/total_num\n",
    "        entropy+=prop *np.log2(prop)\n",
    "    # print('不纯度：', -entropy)\n",
    "    return [-entropy,label_num.items()]\n",
    "\n",
    "# 计算某个特征中某个值的信息熵========================================\n",
    "def calcute_value_entropy(data_train, data_label, index, value):\n",
    "    # data_train 训练样本\n",
    "    # data_label 样本标签\n",
    "    # index，特征索引，就是训练样本中的第几个特征，从0开始。\n",
    "    # value，需要计算信息熵的特征值。某些特征值在样本中只有一个。这种信息熵等于0，就是不纯度为0。\n",
    "    total_num=0   # 某个特征值在数据集中的个数\n",
    "    label_num={}  # 特征值按照分类统计，鸢尾花A类多少个，B类多少个，C类多少个\n",
    "    i=0                # 用来索引data_train与data_label对应位置的数据\n",
    "    # 下面只对index的特征处理，其他特征不处理\n",
    "    for data in data_train[:,index]:\n",
    "        # 只对指定的特征值进行统计\n",
    "        if data == value:\n",
    "            total_num+=1  # 特征值总数递增\n",
    "            # 判定类别\n",
    "            label=data_label[i]\n",
    "            if label not in  label_num:\n",
    "                label_num[label]=0\n",
    "            label_num[label]+=1\n",
    "            \n",
    "        # 标签对应位置下移\n",
    "        i+=1\n",
    "    # 统计特征值的分类个数，下面计算熵\n",
    "    entropy=0.0\n",
    "    for  cls,num in label_num.items():\n",
    "        prop=1.0*num/total_num\n",
    "        entropy+=prop *np.log2(prop)\n",
    "    return [-entropy,total_num,label_num.items()]\n",
    "\n",
    "#  计算某个特征的信息熵========================================\n",
    "def calcute_feature_entropy(data_train, data_label, index):\n",
    "    # data_train 训练样本\n",
    "    # data_label 样本标签\n",
    "    # index，特征索引，就是训练样本中的第几个特征，从0开始。\n",
    "    \n",
    "    total_num=len(data_train)  # \n",
    "    label_entropy={}\n",
    "    # 对训练样本遍历，统计计算每个值的信息熵(只对某个特征值统计)\n",
    "    for data in data_train[ :, index]:\n",
    "        if data not in label_entropy:\n",
    "            ent=calcute_value_entropy(data_train, data_label,index,data)\n",
    "            label_entropy[data]=[ ent[0], ent[1] ]\n",
    "    \n",
    "    # 计算整个特征的信息熵\n",
    "    feature_entropy=0.0\n",
    "    for _, v in  label_entropy.items():\n",
    "        prop=1.0 * v[1] / total_num\n",
    "        feature_entropy+=prop * v[0]\n",
    "    return feature_entropy\n",
    "\n",
    "#  计算特征的信息增益========================================\n",
    "def calcute_gain_value(data_train, data_label):\n",
    "    feature_num=data_train.shape[1]  # 特征数\n",
    "    # 样本总体信息熵\n",
    "    total_entropy=calcute_total_entropy(data_label)\n",
    "    gains=[]\n",
    "    for idx  in range(feature_num):\n",
    "        entropy=calcute_feature_entropy(data,target,idx)\n",
    "        gain=total_entropy[0]-entropy\n",
    "        gains.append(gain)\n",
    "    return gains\n",
    "\n",
    "#  加载数据集========================================\n",
    "data,target=datasets.load_iris(return_X_y=True)    \n",
    "iris_gains=calcute_gain_value(data,target)\n",
    "print(iris_gains)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.  特征选择     \n",
    "&emsp;&emsp;根据上面计算出来的信息增益值的结果，我们可以选择具有最大信息增益值的特征作为子节点分类特征。从上面的计算结果来看，我们选择第**3**个特征来作为分类特征。           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 子节点分类     \n",
    "&emsp;&emsp;我们选择使用第三个特征（index=2）来做子节点分类，下面我们看下第三类特征的特征值熵的情况与信息增益值。并得到观察决策树子节点生成阈值情况：        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2 : [-0.0, 28, dict_items([(0, 28)])]\n",
      "0.4 : [-0.0, 7, dict_items([(0, 7)])]\n",
      "0.3 : [-0.0, 7, dict_items([(0, 7)])]\n",
      "0.1 : [-0.0, 6, dict_items([(0, 6)])]\n",
      "0.5 : [-0.0, 1, dict_items([(0, 1)])]\n",
      "0.6 : [-0.0, 1, dict_items([(0, 1)])]\n",
      "1.4 : [0.5435644431995964, 8, dict_items([(1, 7), (2, 1)])]\n",
      "1.5 : [0.6500224216483541, 12, dict_items([(1, 10), (2, 2)])]\n",
      "1.3 : [-0.0, 13, dict_items([(1, 13)])]\n",
      "1.6 : [0.8112781244591328, 4, dict_items([(1, 3), (2, 1)])]\n",
      "1.0 : [-0.0, 7, dict_items([(1, 7)])]\n",
      "1.1 : [-0.0, 3, dict_items([(1, 3)])]\n",
      "1.8 : [0.41381685030363374, 12, dict_items([(1, 1), (2, 11)])]\n",
      "1.2 : [-0.0, 5, dict_items([(1, 5)])]\n",
      "1.7 : [1.0, 2, dict_items([(1, 1), (2, 1)])]\n",
      "2.5 : [-0.0, 3, dict_items([(2, 3)])]\n",
      "1.9 : [-0.0, 5, dict_items([(2, 5)])]\n",
      "2.1 : [-0.0, 6, dict_items([(2, 6)])]\n",
      "2.2 : [-0.0, 3, dict_items([(2, 3)])]\n",
      "2.0 : [-0.0, 6, dict_items([(2, 6)])]\n",
      "2.4 : [-0.0, 3, dict_items([(2, 3)])]\n",
      "2.3 : [-0.0, 8, dict_items([(2, 8)])]\n"
     ]
    }
   ],
   "source": [
    "from sklearn  import datasets\n",
    "import numpy as np\n",
    "\n",
    "# 计算总体信息熵========================================\n",
    "def calcute_total_entropy(train_label):\n",
    "    # train_label：总体样本的输出标签\n",
    "    # 获取总得样本数\n",
    "    total_num=len(train_label)\n",
    "    # 循环得到所有类别的样本总数\n",
    "    label_num={}\n",
    "    for data in train_label:\n",
    "        if  data not in label_num:\n",
    "            label_num[data]=0\n",
    "        label_num[data]+=1\n",
    "    # 计算每类的概率\n",
    "    entropy=0.0\n",
    "    for  cls,num in label_num.items():\n",
    "        prop=1.0*num/total_num\n",
    "        entropy+=prop *np.log2(prop)\n",
    "    # print('不纯度：', -entropy)\n",
    "    return [-entropy,label_num.items()]\n",
    "\n",
    "# 计算某个特征中某个值的信息熵========================================\n",
    "def calcute_value_entropy(data_train, data_label, index, value):\n",
    "    # data_train 训练样本\n",
    "    # data_label 样本标签\n",
    "    # index，特征索引，就是训练样本中的第几个特征，从0开始。\n",
    "    # value，需要计算信息熵的特征值。某些特征值在样本中只有一个。这种信息熵等于0，就是不纯度为0。\n",
    "    total_num=0   # 某个特征值在数据集中的个数\n",
    "    label_num={}  # 特征值按照分类统计，鸢尾花A类多少个，B类多少个，C类多少个\n",
    "    i=0                # 用来索引data_train与data_label对应位置的数据\n",
    "    # 下面只对index的特征处理，其他特征不处理\n",
    "    for data in data_train[:,index]:\n",
    "        # 只对指定的特征值进行统计\n",
    "        if data == value:\n",
    "            total_num+=1  # 特征值总数递增\n",
    "            # 判定类别\n",
    "            label=data_label[i]\n",
    "            if label not in  label_num:\n",
    "                label_num[label]=0\n",
    "            label_num[label]+=1\n",
    "            \n",
    "        # 标签对应位置下移\n",
    "        i+=1\n",
    "    # 统计特征值的分类个数，下面计算熵\n",
    "    entropy=0.0\n",
    "    for  cls,num in label_num.items():\n",
    "        prop=1.0*num/total_num\n",
    "        entropy+=prop *np.log2(prop)\n",
    "    return [-entropy,total_num,label_num.items()]\n",
    "\n",
    "#  计算某个特征的信息熵========================================\n",
    "def calcute_feature_entropy(data_train, data_label, index):\n",
    "    # data_train 训练样本\n",
    "    # data_label 样本标签\n",
    "    # index，特征索引，就是训练样本中的第几个特征，从0开始。\n",
    "    \n",
    "    total_num=len(data_train)  # \n",
    "    label_entropy={}\n",
    "    # 对训练样本遍历，统计计算每个值的信息熵(只对某个特征值统计)\n",
    "    for data in data_train[ :, index]:\n",
    "        if data not in label_entropy:\n",
    "            ent=calcute_value_entropy(data_train, data_label,index,data)\n",
    "            label_entropy[data]=ent\n",
    "    return label_entropy\n",
    "    \n",
    "#  加载数据集========================================\n",
    "data,target=datasets.load_iris(return_X_y=True)    \n",
    "ents=calcute_feature_entropy(data,target,3)\n",
    "for k,v in ents.items():\n",
    "    print(k,':',v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;对上面不纯度不为0的样本子集继续递归，直到所有节点是叶子节点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. C4.5决策算法      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. C4.5决策算法的思想      \n",
    "&emsp;&emsp;以信息增益进行分类决策时，存在偏向于取值较多的特征的问题（因为总数多，样本概率取值其每个值的概率就小，总体值就小，从而信息增益值就大）。        \n",
    "&emsp;&emsp;        \n",
    "&emsp;&emsp;为了解决这个问题，提出了使用**信息增益比**来作为特征选择的度量方法。    \n",
    "&emsp;&emsp;      \n",
    "&emsp;&emsp;C4.5决策算法是ID3决策算法的优化，优化提现如下几个方面：     \n",
    "&emsp;&emsp;&emsp;&emsp;|-（1）特征选取的依据是：特征信息增益比最大；      \n",
    "&emsp;&emsp;&emsp;&emsp;|-（2）对决策树之后进行剪枝；      \n",
    "&emsp;&emsp;&emsp;&emsp;|-（3）能够处理离散型和连续型的属性类型，就是：将连续型的属性进行离散化处理；      \n",
    "&emsp;&emsp;&emsp;&emsp;|-（4）能够处理具有缺失属性值的训练数据；        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 分裂信息熵      \n",
    "&emsp;&emsp;在介绍特征信息增益比，需要对上面的样本信息熵进行改进：    \n",
    "&emsp;&emsp;&emsp;&emsp;假设某个特征$X$的样本值不同的是$k$个，特征值集合表示为$(x_1,x_2,\\dots,x_k)$，则可以把其中的样本值信息熵改成概率的对数：  \n",
    "&emsp;&emsp;&emsp;&emsp;|-$IS(X) = - \\sum \\limits _{i=1}^{k} p_i log(p_i) ） $        \n",
    "&emsp;&emsp;该信息熵为分裂信息熵，分裂信息熵与样本总体信息熵计算方式一样，只是这儿是对某个特征而言，而且概率计算方式也不同。            \n",
    "&emsp;&emsp;      \n",
    "&emsp;&emsp;下面用代码来说明：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8220180883811645, 4.011709761218928, 5.033829378702226, 4.065662933799395]\n"
     ]
    }
   ],
   "source": [
    "#  计算分裂信息熵   \n",
    "def calcute_split_entropy(data_train,index):\n",
    "    # data_train 训练样本\n",
    "    # data_label 样本标签\n",
    "    # index，特征索引，就是训练样本中的第几个特征，从0开始。\n",
    "    \n",
    "    total_num=len(data_train)   # 某个特征值在数据集中的个数\n",
    "    value_num={}\n",
    "    # 下面只对index的特征处理，其他特征不处理\n",
    "    for data in data_train[:,index]:\n",
    "        if data not in value_num:\n",
    "            value_num[data]=0\n",
    "        value_num[data]+=1\n",
    "    \n",
    "    # 计算分裂熵\n",
    "    entropy=0.0\n",
    "    for _, num in value_num.items():\n",
    "        prop=1.0*num/total_num\n",
    "        entropy+=prop *np.log2(prop)\n",
    "    return -entropy\n",
    "    \n",
    "#  加载数据集========================================\n",
    "data,target=datasets.load_iris(return_X_y=True)    \n",
    "ent=[\n",
    "    calcute_split_entropy(data,0),\n",
    "    calcute_split_entropy(data,1),\n",
    "    calcute_split_entropy(data,2),\n",
    "    calcute_split_entropy(data,3)]\n",
    "# 打印分裂熵\n",
    "print(ent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 特征信息增益比       \n",
    "&ensp;&emsp;信息增益比就是信息增益比与分裂信息熵的比值：     \n",
    "&ensp;&emsp;&ensp;&emsp;|-$IR_{X}=\\dfrac{IG(X)} {IS(X)}$        \n",
    "&ensp;&emsp;    \n",
    "&ensp;&emsp;下面可以计算鸢尾花数据的增益值比。    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8769376208910578, 0.5108699641236061, 1.4463165236458, 1.4358978386754417]\n",
      "[4.8220180883811645, 4.011709761218928, 5.033829378702226, 4.065662933799395]\n",
      "[0.1818611222143841, 0.12734469703221554, 0.28731933779183344, 0.35317680340351865]\n"
     ]
    }
   ],
   "source": [
    "from sklearn  import datasets\n",
    "import numpy as np\n",
    "\n",
    "# 计算总体信息熵========================================\n",
    "def calcute_total_entropy(train_label):\n",
    "    # train_label：总体样本的输出标签\n",
    "    # 获取总得样本数\n",
    "    total_num=len(train_label)\n",
    "    # 循环得到所有类别的样本总数\n",
    "    label_num={}\n",
    "    for data in train_label:\n",
    "        if  data not in label_num:\n",
    "            label_num[data]=0\n",
    "        label_num[data]+=1\n",
    "    # 计算每类的概率\n",
    "    entropy=0.0\n",
    "    for  cls,num in label_num.items():\n",
    "        prop=1.0*num/total_num\n",
    "        entropy+=prop *np.log2(prop)\n",
    "    # print('不纯度：', -entropy)\n",
    "    return [-entropy,label_num.items()]\n",
    "\n",
    "# 计算某个特征中某个值的信息熵========================================\n",
    "def calcute_value_entropy(data_train, data_label, index, value):\n",
    "    # data_train 训练样本\n",
    "    # data_label 样本标签\n",
    "    # index，特征索引，就是训练样本中的第几个特征，从0开始。\n",
    "    # value，需要计算信息熵的特征值。某些特征值在样本中只有一个。这种信息熵等于0，就是不纯度为0。\n",
    "    total_num=0   # 某个特征值在数据集中的个数\n",
    "    label_num={}  # 特征值按照分类统计，鸢尾花A类多少个，B类多少个，C类多少个\n",
    "    i=0                # 用来索引data_train与data_label对应位置的数据\n",
    "    # 下面只对index的特征处理，其他特征不处理\n",
    "    for data in data_train[:,index]:\n",
    "        # 只对指定的特征值进行统计\n",
    "        if data == value:\n",
    "            total_num+=1  # 特征值总数递增\n",
    "            # 判定类别\n",
    "            label=data_label[i]\n",
    "            if label not in  label_num:\n",
    "                label_num[label]=0\n",
    "            label_num[label]+=1\n",
    "            \n",
    "        # 标签对应位置下移\n",
    "        i+=1\n",
    "    # 统计特征值的分类个数，下面计算熵\n",
    "    entropy=0.0\n",
    "    for  cls,num in label_num.items():\n",
    "        prop=1.0*num/total_num\n",
    "        entropy+=prop *np.log2(prop)\n",
    "    return [-entropy,total_num,label_num.items()]\n",
    "\n",
    "#  计算某个特征的信息熵========================================\n",
    "def calcute_feature_entropy(data_train, data_label, index):\n",
    "    # data_train 训练样本\n",
    "    # data_label 样本标签\n",
    "    # index，特征索引，就是训练样本中的第几个特征，从0开始。\n",
    "    \n",
    "    total_num=len(data_train)  # \n",
    "    label_entropy={}\n",
    "    # 对训练样本遍历，统计计算每个值的信息熵(只对某个特征值统计)\n",
    "    for data in data_train[ :, index]:\n",
    "        if data not in label_entropy:\n",
    "            ent=calcute_value_entropy(data_train, data_label,index,data)\n",
    "            label_entropy[data]=[ ent[0], ent[1] ]\n",
    "    \n",
    "    # 计算整个特征的信息熵\n",
    "    feature_entropy=0.0\n",
    "    for _, v in  label_entropy.items():\n",
    "        prop=1.0 * v[1] / total_num\n",
    "        feature_entropy+=prop * v[0]\n",
    "    return feature_entropy\n",
    "\n",
    "#  计算特征的信息增益========================================\n",
    "def calcute_gain_value(data_train, data_label):\n",
    "    feature_num=data_train.shape[1]  # 特征数\n",
    "    # 样本总体信息熵\n",
    "    total_entropy=calcute_total_entropy(data_label)\n",
    "    gains=[]\n",
    "    for idx  in range(feature_num):\n",
    "        entropy=calcute_feature_entropy(data,target,idx)\n",
    "        gain=total_entropy[0]-entropy\n",
    "        gains.append(gain)\n",
    "    return gains\n",
    "#  计算分裂信息熵========================================   \n",
    "def calcute_split_entropy(data_train,index):\n",
    "    # data_train 训练样本\n",
    "    # data_label 样本标签\n",
    "    # index，特征索引，就是训练样本中的第几个特征，从0开始。\n",
    "    \n",
    "    total_num=len(data_train)   # 某个特征值在数据集中的个数\n",
    "    value_num={}\n",
    "    # 下面只对index的特征处理，其他特征不处理\n",
    "    for data in data_train[:,index]:\n",
    "        if data not in value_num:\n",
    "            value_num[data]=0\n",
    "        value_num[data]+=1\n",
    "    \n",
    "    # 计算分裂熵\n",
    "    entropy=0.0\n",
    "    for _, num in value_num.items():\n",
    "        prop=1.0*num/total_num\n",
    "        entropy+=prop *np.log2(prop)\n",
    "    return -entropy\n",
    "\n",
    "def calcute_all_split_entropy(data_train):\n",
    "    feature_num=data_train.shape[1]\n",
    "    ents=[]\n",
    "    for idx in range(feature_num):\n",
    "        ent=calcute_split_entropy(data_train,idx)\n",
    "        ents.append(ent)\n",
    "    return ents\n",
    "#  加载数据集========================================\n",
    "data,target=datasets.load_iris(return_X_y=True)    \n",
    "iris_gains=calcute_gain_value(data,target)\n",
    "iris_split=calcute_all_split_entropy(data)\n",
    "print(iris_gains)\n",
    "print(iris_split)\n",
    "ratio=[]\n",
    "for i in range(len(iris_gains)):\n",
    "    ratio.append(iris_gains[i]/iris_split[i])\n",
    "print(ratio)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;如果使用信息增益比，则分类特征选择第四个特征（\\[0.1818611222143841, 0.12734469703221554, 0.28731933779183344, `0.35317680340351865`\\]）。由于评估了分裂度，对某个特征中值比较多的情况，使用分类信息熵作为分母，可以同比降低这种影响。        \n",
    "&emsp;&emsp;计算结果与sklearn一致。      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.  离散化处理       \n",
    "&emsp;&emsp;如果按照上面的方式，对连续值得情况会出现很多子节点。为了避免这种情况，需要进行离散化处理。 离散处理的思路相对比较简单：        \n",
    "&emsp;&emsp;假设某个分类特征具有N个值。     \n",
    "&emsp;&emsp;&emsp;&emsp;|-（1）对某个分类特征的所有值进行排序（升序），得到$(x_1,x_2,\\dots,x_N)$。           \n",
    "&emsp;&emsp;&emsp;&emsp;|-（2）对相邻两个值取平均值，并使用这个平均值作为分类阈值$\\bar{x}=\\dfrac{x_i + x_{i+1}}{2}$，把特征的值分成两类，A={$x_i | x_i>\\bar{x}$} 与B={$x_i | x_i \\le \\bar{x}$}    \n",
    "&emsp;&emsp;&emsp;&emsp;|-（3）计算按照A、B分成两类的分类增益值，这样得到N-1个增益值；     \n",
    "&emsp;&emsp;&emsp;&emsp;|-（4） 取信息增益值最大那个$\\bar{x}$作为分类阈值。         \n",
    "&emsp;&emsp;       \n",
    "&emsp;&emsp;其中分类增益值计算公式为：  \n",
    "&emsp;&emsp;假设：A类的格式是$N_A$,B类的个数是$N_B$，则信息增益为：$I=\\dfrac{N_A}{N} H_A + \\dfrac{N_B}{N} H_B$     \n",
    "&emsp;&emsp; 下面使用代码实现计算鸢尾花第一个节点的阈值，分类特征使用上面的计算结果，使用第四个特征。       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意：对于离散特征值，也可以采用二分的分组模式，每个分组的增益值最大，作为最终分组来划分直接点。   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15000000000000002 : 0.14906466204571436\n",
      "0.25 : 0.1490646620457144\n",
      "0.35 : 0.1490646620457144\n",
      "0.45 : 0.14906466204571436\n",
      "0.55 : 0.1490646620457144\n",
      "0.8 : 0.14906466204571434\n",
      "1.05 : 0.1490646620457144\n",
      "1.15 : 0.1490646620457144\n",
      "1.25 : 0.14906466204571436\n",
      "1.35 : 0.14906466204571436\n",
      "1.45 : 0.14906466204571436\n",
      "1.55 : 0.1490646620457144\n",
      "1.65 : 0.1490646620457144\n",
      "1.75 : 0.14906466204571436\n",
      "1.85 : 0.1490646620457144\n",
      "1.95 : 0.1490646620457144\n",
      "2.05 : 0.14906466204571436\n",
      "2.1500000000000004 : 0.1490646620457144\n",
      "2.25 : 0.1490646620457144\n",
      "2.3499999999999996 : 0.14906466204571436\n",
      "2.45 : 0.14906466204571436\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "def calcute_entrop(train_label):\n",
    "    # 统计类别\n",
    "    category_num={}\n",
    "    for label in train_label:\n",
    "        if label not in  category_num:\n",
    "            category_num[label]=0\n",
    "        category_num[label]+=1\n",
    "    \n",
    "    # 开始计算基尼系数\n",
    "    total_num=len(train_label)\n",
    "    gini=0.0\n",
    "    for _,num in  category_num.items():\n",
    "        prop=1.0 * num/total_num\n",
    "        gini+=prop * ( 1.0 - prop )\n",
    "    return gini\n",
    "\n",
    "def  calcute_gini_split(train_data,train_label,index):\n",
    "    # 找到某个特征最大基尼系数的切分\n",
    "    data=np.hstack((train_data,train_label.reshape(train_label.shape[0],1)))\n",
    "    \n",
    "    total_num=len(train_label)\n",
    "    gini={}\n",
    "    # 对index特征划分数据集\n",
    "    data_set=list(set(data[:,index]))\n",
    "    data_set=sorted(data_set)\n",
    "    for idx in range(len(data_set)-1):\n",
    "        x_=(data_set[idx] + data_set[idx+1]) /2.0\n",
    "        new_data = data[data[:,index].argsort()] \n",
    "        data1=new_data[new_data[:,index]<=x_]\n",
    "        data2=new_data[new_data[:,index]>x_]\n",
    "        gini1=calcute_gini(data1[:,-1])\n",
    "        gini2=calcute_gini(data2[:,-1])\n",
    "        gini_=1.0* len(data1) / total_num * gini1 + 1.0* len(data2) / total_num * gini2\n",
    "        gini[x_]=[gini_,len(data1), len(data2)]\n",
    "    return gini\n",
    "        \n",
    "data,target=datasets.load_iris(return_X_y=True)    \n",
    "#gini=calcute_gini_split(data,target,3)\n",
    "#for k,v in gini.items():\n",
    "#    print(k,':',v)\n",
    "# ====最小基尼系数\n",
    "#re=min(gini.items(),key=lambda x:x[1])\n",
    "for idx in range(data.shape[1]):\n",
    "    gini=calcute_gini_split(data,target,idx)\n",
    "    re=min(gini.items(),key=lambda x:x[1][0])\n",
    "    print(re)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 剪枝   \n",
    "&emsp;&emsp;由于决策树的构建完全是依赖于训练样本，因此该决策树对训练样本能够产生完美的拟合效果。但这样的决策树容易过拟合。      \n",
    "&emsp;&emsp;解决过拟合问题的解决方案就是剪枝。，剪枝方法分为预剪枝和后剪枝两大类。   \n",
    "&emsp;&emsp;&emsp;&emsp;|-预剪枝：是在构建决策树的过程中，终止决策树分支，从而避免过多的节点产生。预剪枝方法虽然简单但实用性不强，因为很难精确的判断何时终止树的生长。    \n",
    "&emsp;&emsp;&emsp;&emsp;|-后剪枝：是在决策树构建完成之后，对那些置信度不达标的节点子树用叶子结点代替，该叶子结点的类标号用该节点子树中频率最高的类标记。   \n",
    "&emsp;&emsp;     \n",
    "&emsp;&emsp;常见的后剪枝方法有：    \n",
    "&emsp;&emsp;&emsp;&emsp;|-CCP(Cost Complexity Pruning)       \n",
    "&emsp;&emsp;&emsp;&emsp;|-REP(Reduced Error Pruning)     \n",
    "&emsp;&emsp;&emsp;&emsp;|-PEP(Pessimistic Error Pruning)    \n",
    "&emsp;&emsp;&emsp;&emsp;|-MEP(Minimum Error Pruning)    \n",
    "&emsp;&emsp;剪枝的算法这里不介绍。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.CART决策算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;CART算法与C4.5差不多，主要CART采用的是基尼系数。   对一个数据集来说，基尼系数计算方式为：     \n",
    "&emsp;&emsp;假设有$K$个类别，第$k$个类别的概率为$p_k$      \n",
    "&emsp;&emsp;&emsp;&emsp;|- $Gini(p) = \\sum\\limits_{k=1}^{K}p_k(1-p_k) = 1- \\sum\\limits_{k=1}^{K}p_k^2$     \n",
    "&emsp;&emsp;而且采用的是把数据集按照某个特征$A$分成两部分处理，假设训练样本是$D$，则可以分成两个数据集$D_1,D_2$ ,则A特征在该划分模式下的基尼系数为：      \n",
    "&emsp;&emsp;&emsp;&emsp;|- $Gini(D,A) = \\frac{|D_1|}{|D|}Gini(D_1) + \\frac{|D_2|}{|D|}Gini(D_2)$    \n",
    "&emsp;&emsp;  \n",
    "&emsp;&emsp;特征选择：所有二分中基尼系数最小的特征与对应的二分特征值。             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. sklearnCART决策算法体验     \n",
    "sklearn实现的决策算法使用的就是CART算法，其典型结果就是采用二叉树。      \n",
    "下面是skleran二叉树决策算法的可视化结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split    # 训练集切分\n",
    "import sklearn\n",
    "from sklearn import tree\n",
    "from sklearn.externals.six import StringIO    #数据缓冲\n",
    "import pydot    # 把数据转换为图像\n",
    "\n",
    "data,target=datasets.load_iris(return_X_y=True)\n",
    "# data,target=datasets.load_linnerud(return_X_y=True)\n",
    "# data=data[0:100]\n",
    "# target=target[0:100]\n",
    "\n",
    "dtc=DecisionTreeClassifier(random_state=0,criterion='gini')    # 为了保证输出结果的稳定性，需要random_state固定。\n",
    "dtc.fit(data,target)  \n",
    "\n",
    "buff = StringIO()\n",
    "# tree.export_graphviz(dtc,out_file=buff)     # export_graphviz只能导出数据\n",
    "tree.export_graphviz(dtc, out_file=buff, feature_names=['特征1','特征2','特征3','特征4'], class_names=['A类','B类','C类'], filled=True, rounded=True, special_characters=True)  \n",
    "#tree.export_graphviz(dtc, out_file=buff, feature_names=['特征1','特征2','特征3','特征4'], class_names=['A类','B类'], filled=True, rounded=True, special_characters=True)  \n",
    "(graph,)= pydot.graph_from_dot_data(buff.getvalue())   # 转换为图像\n",
    "graph.write_svg(\"iris_gini.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![iris_gini.svg](iris_gini.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 计算总体Gini系数     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666667\n",
      "0.5\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "def calcute_gini(train_label):\n",
    "    # 统计类别\n",
    "    category_num={}\n",
    "    for label in train_label:\n",
    "        if label not in  category_num:\n",
    "            category_num[label]=0\n",
    "        category_num[label]+=1\n",
    "    \n",
    "    # 开始计算基尼系数\n",
    "    total_num=len(train_label)\n",
    "    gini=0.0\n",
    "    for _,num in  category_num.items():\n",
    "        prop=1.0 * num/total_num\n",
    "        gini+=prop * ( 1.0 - prop )\n",
    "    return gini\n",
    "\n",
    "# 加载数据集\n",
    "_,target=datasets.load_iris(return_X_y=True)\n",
    "print(calcute_gini(target))\n",
    "print(calcute_gini(target[0:100]))\n",
    "print(calcute_gini(target[0:50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 计算一个数据集分成两个子集的基尼系数          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5.45, [0.4389063317634746, 52, 98])\n",
      "(3.3499999999999996, [0.5462962962962963, 114, 36])\n",
      "(2.45, [0.3333333333333333, 50, 100])\n",
      "(0.8, [0.3333333333333333, 50, 100])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "def calcute_gini(train_label):\n",
    "    # 统计类别\n",
    "    category_num={}\n",
    "    for label in train_label:\n",
    "        if label not in  category_num:\n",
    "            category_num[label]=0\n",
    "        category_num[label]+=1\n",
    "    \n",
    "    # 开始计算基尼系数\n",
    "    total_num=len(train_label)\n",
    "    gini=0.0\n",
    "    for _,num in  category_num.items():\n",
    "        prop=1.0 * num/total_num\n",
    "        gini+=prop * ( 1.0 - prop )\n",
    "    return gini\n",
    "\n",
    "def  calcute_gini_split(train_data,train_label,index):\n",
    "    # 找到某个特征最大基尼系数的切分\n",
    "    data=np.hstack((train_data,train_label.reshape(train_label.shape[0],1)))\n",
    "    \n",
    "    total_num=len(train_label)\n",
    "    gini={}\n",
    "    # 对index特征划分数据集\n",
    "    data_set=list(set(data[:,index]))\n",
    "    data_set=sorted(data_set)\n",
    "    for idx in range(len(data_set)-1):\n",
    "        x_=(data_set[idx] + data_set[idx+1]) /2.0\n",
    "        new_data = data[data[:,index].argsort()] \n",
    "        data1=new_data[new_data[:,index]<=x_]\n",
    "        data2=new_data[new_data[:,index]>x_]\n",
    "        gini1=calcute_gini(data1[:,-1])\n",
    "        gini2=calcute_gini(data2[:,-1])\n",
    "        gini_=1.0* len(data1) / total_num * gini1 + 1.0* len(data2) / total_num * gini2\n",
    "        gini[x_]=[gini_,len(data1), len(data2)]\n",
    "    return gini\n",
    "        \n",
    "data,target=datasets.load_iris(return_X_y=True)    \n",
    "#gini=calcute_gini_split(data,target,3)\n",
    "#for k,v in gini.items():\n",
    "#    print(k,':',v)\n",
    "# ====最小基尼系数\n",
    "#re=min(gini.items(),key=lambda x:x[1])\n",
    "for idx in range(data.shape[1]):\n",
    "    gini=calcute_gini_split(data,target,idx)\n",
    "    re=min(gini.items(),key=lambda x:x[1][0])\n",
    "    print(re)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 四、决策树的使用     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当样本根据决策树规则，路由到最后叶子节点的时候，如果最后节点不纯度为0，则直接返回类别，如果不纯度不为0，则返回概率最大的类别。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 回归  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当样本根据决策树规则，路由到最后叶子节点的时候，返回叶子节点的平均值或者中间值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 五、决策树算法实现\n",
    "下面代码是直接抄袭别人的，实现的是ID3算法。\n",
    "如果上面思路清楚后，实际实现二叉树相对来说思维比较简单。     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3: {0.2: 0.0, 0.4: 0.0, 0.3: 0.0, 0.5: 0.0, 0.6: 0.0, 1.4: {2: {3.9: 1.0, 4.8: 1.0, 4.7: 1.0, 4.4: 1.0, 4.6: 1.0, 5.6: 2.0}}, 1.5: {2: {4.7: 1.0, 4.2: 1.0, 4.5: 1.0, 4.9: 1.0, 4.6: 1.0, 5.0: 2.0, 5.1: 2.0}}, 1.3: 1.0, 1.6: {0: {7.2: 2.0, 6.3: 1.0, 6.0: 1.0}}, 1.0: 1.0, 1.1: 1.0, 2.5: 2.0, 2.0: 2.0, 2.1: 2.0, 1.2: 1.0, 1.7: {0: {4.9: 2.0, 6.7: 1.0}}, 0.1: 0.0, 2.2: 2.0, 2.3: 2.0, 1.8: {1: {2.5: 2.0, 3.0: 2.0, 2.9: 2.0, 3.2: {0: {5.9: 1.0, 7.2: 2.0}}, 2.7: 2.0, 2.8: 2.0, 3.1: 2.0}}, 1.9: 2.0, 2.4: 2.0}}\n"
     ]
    }
   ],
   "source": [
    "from sklearn  import datasets\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "def calcShannonEnt(trainData):\n",
    "    numEntries = len(trainData)\n",
    "    labelDic = {}\n",
    "    for trainLine in trainData:\n",
    "        currentLabel = trainLine[-1]\n",
    "        if currentLabel not in labelDic:\n",
    "            labelDic[currentLabel] = 0\n",
    "        labelDic[currentLabel] += 1\n",
    "\n",
    "    shannonEnt = 0.0\n",
    "    for key,value in labelDic.items():\n",
    "        prob = float(value)/numEntries\n",
    "        shannonEnt -= prob * math.log(prob,2)\n",
    "    return shannonEnt\n",
    "\n",
    "def splitData(trainData,index,value):\n",
    "    subData = []\n",
    "    for trainLine in trainData:\n",
    "        if trainLine[index]==value:\n",
    "            reducedFeatVec = []\n",
    "            for i in range(0,len(trainLine),1):\n",
    "                if i==index:\n",
    "                    continue\n",
    "                reducedFeatVec.append(trainLine[i])\n",
    "            subData.append(reducedFeatVec)\n",
    "    #print('subData',subData)\n",
    "    #print(len(trainData),len(subData))\n",
    "    return subData\n",
    "\n",
    "def chooseBestFeature(trainData):\n",
    "    numFeatures = len(trainData[0])-1\n",
    "    baseEntropy = calcShannonEnt(trainData)\n",
    "    bestInfoGain = 0.0\n",
    "    bestFeature = -1\n",
    "    for i in range(0,numFeatures,1):\n",
    "        currentFeature = [temp[i] for temp in trainData]\n",
    "        uniqueValues = set(currentFeature)\n",
    "        newEntropy = 0.0\n",
    "        splitInfo = 0.0\n",
    "        for value in uniqueValues:\n",
    "            subData = splitData(trainData,i,value)\n",
    "            prob = float(len(subData))/len(trainData)\n",
    "            newEntropy += prob * calcShannonEnt(subData)\n",
    "            splitInfo -= prob * math.log(prob,2)\n",
    "        infoGain = (baseEntropy - newEntropy) / splitInfo\n",
    "        if infoGain > bestInfoGain :\n",
    "            bestInfoGain = infoGain\n",
    "            bestFeature = i\n",
    "    return bestFeature\n",
    "\n",
    "\n",
    "def CreateTree(trainData):\n",
    "    classList = [temp[-1] for temp in trainData]\n",
    "    classListSet = set(classList)\n",
    "    if len(classListSet)==1:\n",
    "        return classList[0]\n",
    "    if len(trainData[0])==1:\n",
    "        return majorityCnt(classList)\n",
    "\n",
    "    bestFeature = chooseBestFeature(trainData)\n",
    "    myTree = {bestFeature:{}}\n",
    "    featureValues = [example[bestFeature] for example in trainData]\n",
    "    uniqueValues = set(featureValues)\n",
    "    for value in uniqueValues:\n",
    "        myTree[bestFeature][value] = CreateTree(splitData(trainData, bestFeature, value))\n",
    "    return myTree\n",
    "\n",
    "\n",
    "def app_test(data_train,label_train,index):\n",
    "    # data_train：离散化数据集\n",
    "    # label_train: 训练数据集的标签\n",
    "    # index：离散化特征\n",
    "    \n",
    "    # 为了避免排序产生的错乱，合并数据集与标签集\n",
    "    data = np.hstack((data_train,label_train.reshape(label_train.shape[0],1)))\n",
    "    # 按照指定特征排序（按照index排序）\n",
    "    return CreateTree(data)\n",
    "    \n",
    "#  加载数据集========================================\n",
    "data,target=datasets.load_iris(return_X_y=True)    \n",
    "ent=app_test(data,target,3)\n",
    "print(ent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 四、决策树相关的集成算法 \n",
    "&emsp;&emsp;绝决策树与其他算法集成产生如下集成算法    \n",
    "&emsp;&emsp;&emsp;&emsp;|-（1）Bagging + 决策树 = 随机森林    \n",
    "&emsp;&emsp;&emsp;&emsp;|-（2）AdaBoost + 决策树 = 提升树      \n",
    "&emsp;&emsp;&emsp;&emsp;|-（3）Gradient Boosting + 决策树 = GBDT     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 随机森林算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（1）从原始训练集中使用Bootstraping方法随机有放回采样选出m个样本       \n",
    "（2）对m个样本采用决策树（也可以使用其他分类算法）建立分类器      \n",
    "（3）重复n次，产生n个分类器    \n",
    "（4）把测试样本对n个分类器执行运算，所以分类的结果中，选择分类器最多的类别为最终输出；    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9166666666666666\n",
      "[0 2 2 1 1 1 0 0 0 0 2 2 2 0 1 2 1 1 2 0 2 0 2 2 2 2 2 1 2 2 2 1 2 2 1 1 1\n",
      " 2 2 1 0 0 1 0 2 0 1 2 1 1 2 1 1 2 1 0 2 1 0 0]\n",
      "[0 2 2 1 2 1 0 0 0 0 2 1 2 0 1 2 1 1 2 0 2 0 2 2 2 2 2 1 2 2 1 1 2 2 1 1 1\n",
      " 2 2 1 0 0 1 0 2 0 1 2 1 1 2 1 1 1 1 0 1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn  import datasets\n",
    "from sklearn.model_selection import train_test_split    # 训练集切分\n",
    "\n",
    "data,target=datasets.load_iris(return_X_y=True)\n",
    "train_data, test_data,train_target, test_target = train_test_split(data, target, test_size=0.4,shuffle=True)\n",
    "\n",
    "# 使用训练样本训练\n",
    "rfc = RandomForestClassifier(n_estimators=100,random_state=0)\n",
    "data,target=datasets.load_iris(return_X_y=True)   \n",
    "rfc.fit(train_data,train_target)\n",
    "print(rfc.score(test_data,test_target))\n",
    "print(rfc.predict(test_data))\n",
    "print(test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
