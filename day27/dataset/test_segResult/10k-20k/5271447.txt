岗位职责 ： 
 1 、 参与 / 负责 开发 分布式 爬虫 框架 ， 管理 分布式 爬虫 服务器 ， 开发 任务调度 引擎 ； 
 2 、 负责 设计 和 开发 分布式 网络 爬虫 ， 进行 多 平台 、 多 终端 信息 的 抓取 和 分析 。 
 3 、 设计 爬虫 策略 和 防 屏蔽 规则 ， 解决 封 账号 、 封 IP 、 验证码 等 难点 攻克 ； 
 4 、 系统 解决 动态 网页内容 抓取 、 深度 网页内容 抓取 ； 
 5 、 实现 大规模 文本 、 图像 、 视频 数据 的 抓取 及 数据 清洗 工作 。 
 
 任职 要求 ： 
 1 、   本科 及 以上学历 ， 计算机 等 相关 专业 ，   2 年 及 以上 工作 经验 ； 
 2 、   精通 Python 语言 ，   有过 Python 相关 的 开发 经验 ； 
 3 、   熟悉 网页 抓取 原理 及 技术 ， 能够 总结 分析 不同 网站 ， 网页 的 结构 特点 及 规律 ； 
 4 、   熟悉 常用 的 Python 爬虫 框架 以及 其 分布式 爬取 开发 
 5 、   熟悉 反爬 策略 的 应对 ， 能够 解决 封 账号 、 封 IP 采集 等 问题 ； 
 6 、   熟悉 Appium 、 Selenium 、 PhantomJS   、 WebDriver 等 技术 ； 
 7 、   熟悉 Mysql 、 redis 等 数据库 ， 有过 数据库 调优 和 海量 数据 存储 经验 者 优先 。 
 
 加 分项 （ 满足 其一 即可 ） ： 
 1 、 有 验证码 破解 ， 反爬 ， 分布式 爬虫 架构 ， 数据挖掘 ， 搭建 数据仓库 经验 ； 
 2 、 有 知名 app 或者 网站   ( Facebook ,   Linkedin ,   Twitter ,   Amazon ,   淘宝 ， 微博 等 ） 抓取 经验 ； 
 3 、 熟悉 大 数据 相关 技术 ， Hadoop 、 Kafka 、 Elasticsearch 、 HBase 等 优先 ； 
 4 、 强烈 的 责任心 和 团队 合作 能力 ， 性格开朗 ， 善于 沟通 ， 自驱 学习 ， 逻辑思维 能力 并且 敢于创新 和 接受 挑战 。